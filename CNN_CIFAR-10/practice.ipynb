{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice Prior to Implementing the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple ways of creating 2D convolution; unnamed integer parameters: **1)** number of color channels of a given input image, **2)** number of output channels (how many filters we want from the given layer), **3)** kernel size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 4), stride=(3, 3), padding=(1, 2))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv2d(3, 16, 3)                 # applying 2D convolution to an img\n",
    "nn.Conv2d(3, 16, 3, padding=1)      # padding added\n",
    "nn.Conv2d(3, 16, (3, 4), padding=1) # non square kernel\n",
    "nn.Conv2d(3, 16, 3, stride=2)       # stride added\n",
    "nn.Conv2d(3, 16, (3, 4), \n",
    "          stride=(3, 3),\n",
    "          padding=(1, 2))           # non square stride and padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reasons pooling layers are used: reduction in the number of computations, prevention of overfitting, positional invariance (i. e. recognition of a feature regardless of its position in the image).\n",
    "\n",
    "Integer paramater passed to pooling layers is the size of the square kernel used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2., 8., 7., 1., 1.],\n",
      "         [0., 6., 8., 2., 6.],\n",
      "         [8., 1., 1., 4., 3.],\n",
      "         [2., 9., 1., 0., 4.],\n",
      "         [8., 3., 0., 8., 8.]],\n",
      "\n",
      "        [[8., 4., 2., 7., 0.],\n",
      "         [2., 9., 4., 9., 2.],\n",
      "         [8., 0., 9., 6., 8.],\n",
      "         [3., 5., 2., 9., 8.],\n",
      "         [3., 2., 4., 1., 3.]],\n",
      "\n",
      "        [[8., 9., 8., 0., 0.],\n",
      "         [4., 1., 5., 5., 1.],\n",
      "         [7., 7., 2., 4., 2.],\n",
      "         [0., 8., 0., 7., 7.],\n",
      "         [6., 3., 8., 4., 7.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[4.5556, 4.2222, 3.6667],\n",
       "         [4.0000, 3.5556, 3.2222],\n",
       "         [3.6667, 3.0000, 3.2222]],\n",
       "\n",
       "        [[5.1111, 5.5556, 5.2222],\n",
       "         [4.6667, 5.8889, 6.3333],\n",
       "         [4.0000, 4.2222, 5.5556]],\n",
       "\n",
       "        [[5.6667, 4.5556, 3.0000],\n",
       "         [3.7778, 4.3333, 3.6667],\n",
       "         [4.5556, 4.7778, 4.5556]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_pool = nn.MaxPool2d(3, stride=1)            # define a max pool\n",
    "a = torch.FloatTensor(3, 5, 5).random_(0, 10)   # tensor to perform on\n",
    "# print(a)\n",
    "# max_pool(a)\n",
    "\n",
    "avg_pool = nn.AvgPool2d(3, stride=1)            # define average pool\n",
    "a = torch.FloatTensor(3, 5, 5).random_(0, 10)\n",
    "print(a)\n",
    "avg_pool(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring various transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    CenterCrop(size=(10, 10))\n",
       "    ToTensor()\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transforms.ToTensor()               # tensor from the image\n",
    "transforms.Normalize((0.5), (0.5))  # normalization\n",
    "transforms.Resize((10, 10))\n",
    "transforms.CenterCrop(10)           # cropping\n",
    "transforms.Pad(1, 0)                # padding\n",
    "transforms.Pad((1, 2, 2, 3), padding_mode='reflect')\n",
    "\n",
    "transforms.Compose([                # chaining transforms\n",
    "    transforms.CenterCrop(10),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring data augmentation for computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    RandomRotation(degrees=(-10, 10), resample=False, expand=False)\n",
       "    ToTensor()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.RandomCrop(10)\n",
    "transforms.RandomCrop((10, 20))\n",
    "\n",
    "transforms.RandomHorizontalFlip(p=0.3)\n",
    "transforms.RandomVerticalFlip(p=0.3)\n",
    "\n",
    "transforms.ColorJitter(             # color, contrast, saturation, hue\n",
    "    0.25, 0.25, 0.25, 0.25)\n",
    "\n",
    "transforms.RandomRotation(10)\n",
    "\n",
    "transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
