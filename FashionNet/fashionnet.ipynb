{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /Users/nikolavetnic/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9fb094952b4ae09fb16698f228a2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/nikolavetnic/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /Users/nikolavetnic/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /Users/nikolavetnic/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0f4223ca444c2da174cef5ab1ddcef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/nikolavetnic/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /Users/nikolavetnic/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /Users/nikolavetnic/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f941e244f44b77ae1f34aa5e0c672f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/nikolavetnic/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to /Users/nikolavetnic/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to /Users/nikolavetnic/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ad12460cd24e0aa9462db952c1ae4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/nikolavetnic/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to /Users/nikolavetnic/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "Processing...\n",
      "\n",
      "\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "trainset = datasets.FashionMNIST(\n",
    "    '~/.pytorch/F_MNIST_data/',\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence `__init__` defines the component parts and `forward()` chains them together.\n",
    "\n",
    "To add nonlinear capabilities to `nn.Linear()` layers (which are basically linear functions defined as *x * W + B*) `nn.ReLU()` activator is used. Final layer is fed to `nn.Softmax()` activator since a single output class with higher probability than all others is needed (while all output probabilities should sum up to 1).\n",
    "\n",
    "Parameter `dim=1` in `self.softmax = nn.Softmax(dim=1)` ensures that `Softmax` is taken across the columns of the output.\n",
    "\n",
    "EDIT 1: Subsequently `Softmax` is changed to `LogSoftmax()`.\n",
    "\n",
    "EDIT 2: The following cell contains two different ways to implement the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class FashionNetwork(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.hidden1 = nn.Linear(784, 256)\n",
    "#         self.hidden2 = nn.Linear(256, 128)\n",
    "#         self.output = nn.Linear(128, 10)\n",
    "#         # self.softmax = nn.Softmax(dim=1)\n",
    "#         self.log_softmax = nn.LogSoftmax()\n",
    "#         self.activation = nn.ReLU()\n",
    "#         self.drop = nn.Dropout(p=0.25)\n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(784, 256)\n",
    "        self.hidden2 = nn.Linear(256, 128)\n",
    "        self.output = nn.Linear(128, 10)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.hidden1(x)\n",
    "#         x = self.activation(x)\n",
    "#         x = self.drop(x)\n",
    "#         x = self.hidden2(x)\n",
    "#         x = self.activation(x)\n",
    "#         x = self.drop(x)\n",
    "#         x = self.output(x)\n",
    "#         # output = self.softmax(x)\n",
    "#         output = self.log_softmax(x)\n",
    "#         return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.log_softmax(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FashionNetwork(\n",
      "  (hidden1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (hidden2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = FashionNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Jibin Mathew, PyTorch Artificial Intelligence Fundamentals (2020), p34*: \n",
    "\n",
    "\"...In this recipe, we replaced softmax with log softmax so that we could then use the log of probabilities over probabilities, which has nice theoretic interpretations. There are various reasons for doing this, including improved numerical performance and gradient optimization. These advantages can be extremely important when training a model that can be computationally challenging and expensive. Furthermore, it has a high penalizing effect when it is not predicting the correct class.\n",
    "\n",
    "We therefore use negative log likelihood when dealing with log softmax, as softmax is not compatible. It is useful in classification between n number of classes. The log would ensure that we are not dealing with very small values between 0 and 1, and negative values would ensure that a logarithm of probability that is less than 1 is nonzero. Our goal would be to reduce this negative log loss error function. In PyTorch, the loss function is called a criterion, and so we named our loss function `criterion`.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Jibin Mathew, PyTorch Artificial Intelligence Fundamentals (2020), p35*:\n",
    "\n",
    "\"...Optimizers can be thought of as fiddling with the model weights to get the best possible model based on the difference in prediction from the model and the actual output, and the loss function acts as a guide by indicating when the optimizer is going right or wrong.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.003,\n",
       " 'betas': (0.9, 0.999),\n",
       " 'eps': 1e-08,\n",
       " 'weight_decay': 0,\n",
       " 'amsgrad': False}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-3)\n",
    "optimizer.defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `image.view(image.shape[0], -1)` each tensor is reshaped from 64x28x28 (64 images in a batch, where each image is 28x28) to 64x784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolavetnic/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.4975\n",
      "Training loss: 0.3842\n",
      "Training loss: 0.3515\n",
      "Training loss: 0.3264\n",
      "Training loss: 0.3126\n",
      "Training loss: 0.2970\n",
      "Training loss: 0.2864\n",
      "Training loss: 0.2767\n",
      "Training loss: 0.2701\n",
      "Training loss: 0.2608\n"
     ]
    }
   ],
   "source": [
    "epoch = 10\n",
    "\n",
    "for _ in range(epoch):\n",
    "    running_loss = 0\n",
    "    for image, label in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        image = image.view(image.shape[0], -1)\n",
    "        pred = model(image)\n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f'Training loss: {running_loss/len(trainloader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Jibin Mathew, PyTorch Artificial Intelligence Fundamentals (2020), p39*:\n",
    "\n",
    "\"...Dropouts can be intuitively understood as creating a large number of ensemble models, learning to capture various features under one big definition of a model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
