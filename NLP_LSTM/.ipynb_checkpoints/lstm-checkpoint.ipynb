{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory Network\n",
    "\n",
    "*Jibin Mathew, PyTorch Artificial Intelligence Fundamentals (2020), p82*:\n",
    "\n",
    "\"...LSTM networks are a type of RNNs that has internal gates that helps in better information persistence. These gates are tiny neural networks that control when information needs to be saved and when it can be erased or forgotten. RNNs suffer from vanishing and exploding gradients, making it difficult to learn long-term dependencies. LSTMs are resistant to exploding and vanishing gradients, although it is still mathematically possible.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'test', 'for', 'the', 'tokenizer.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = lambda words: words.split()\n",
    "tokenizer(\"This is a test for the tokenizer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.data import Field\n",
    "\n",
    "REVIEW = Field(sequential=True, tokenize=tokenizer, lower=True)\n",
    "LABEL = Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import TabularDataset\n",
    "\n",
    "train_datafields = [\n",
    "    (\"id\", None),\n",
    "    (\"content\", REVIEW),\n",
    "    (\"Business\", LABEL),\n",
    "    (\"SciTech\", LABEL),\n",
    "    (\"Sports\", LABEL),\n",
    "    (\"World\", LABEL)]\n",
    "\n",
    "test_datafields = [\n",
    "    (\"id\", None),\n",
    "    (\"content\", REVIEW)]\n",
    "\n",
    "train, valid = TabularDataset.splits(\n",
    "    path='/Users/nikolavetnic/Desktop/Text Materials/DeepLearning/[AI] Jibin Mathew - PyTorch Artificial Intelligence Fundamentals (2020)/Chapter 4/',\n",
    "    train='train.csv',\n",
    "    validation='valid.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=train_datafields)\n",
    "\n",
    "test = TabularDataset.splits(\n",
    "    path='/Users/nikolavetnic/Desktop/Text Materials/DeepLearning/[AI] Jibin Mathew - PyTorch Artificial Intelligence Fundamentals (2020)/Chapter 4/',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=test_datafields)\n",
    "\n",
    "REVIEW.build_vocab(train, min_freq=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing iterators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import BucketIterator\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter, test_iter = BucketIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    sort_key=lambda x: len(x.comment_text),\n",
    "    sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./vec/glove_embedding/glove.6B.zip: 862MB [06:48, 2.11MB/s]                               \n",
      "100%|█████████▉| 399441/400000 [00:29<00:00, 13623.07it/s]"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "\n",
    "# loading the pretrained embedding vectors\n",
    "vec = Vectors(\n",
    "    'glove.6B.100d.txt',\n",
    "    cache='./vec/glove_embedding/',\n",
    "    url='http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "\n",
    "REVIEW.build_vocab(train, min_freq=2, vectors=vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an LSTM network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|█████████▉| 399441/400000 [00:40<00:00, 13623.07it/s]"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(REVIEW.vocab), embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden)\n",
    "    \n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "    \n",
    "model = LSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a multilayer LSTM\n",
    "\n",
    "*Jibin Mathew, PyTorch Artificial Intelligence Fundamentals (2020), p86*:\n",
    "\n",
    "\"...In this recipe, we added `num_layers` and the parameter in the constructor to control the number of layers of LSTMs in the model, and passed it as a keyword argument, `num_layers`, in the LSTM definition.\n",
    "\n",
    "Then, in the `forward()` method, we took the hidden state only from the last LSTM layer using `hidden[-1]` since the shape of the hidden state is `[num_layers * num_directions, batch, hidden_dim]`, where `num_direction` is `1` by default. This meant that `hidden[-1]` gave the last layer's hidden state. By doing this, we could choose `num_layers` as a hyperparameter. The hidden state output from the lower layer was passed as the input of the higher state.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLSTMClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(REVIEW.vocab), embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        hidden = self.dropout(hidden)\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "model = MultiLSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT, NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a bidirectional LSTM\n",
    "\n",
    "*Jibin Mathew, PyTorch Artificial Intelligence Fundamentals (2020), p88*:\n",
    "\n",
    "\"...In this recipe, we set the `bidirectional` flag to `True` in the LSTM definition. We concatenated the hidden states of the forward and backward LSTMs and passed them into the fully connected layer. Because of this, the input dimension of the fully connected layer was doubled to accommodate the forward and backward hidden state tensors.\n",
    "\n",
    "In the `forward()` method, we concatenated the forward and backward hidden states using `torch.cat()`, and we used the last hidden states of the forward and backward LSTMs. In PyTorch, the hidden states are stacked as `[forward_layer_0, backward_layer_0, forward_layer_1, backward_layer_1, ..., forward_layer_n, backward_layer_n]`, and so the required tensors are `hidden[-2,:,:]`, `hidden[-1,:,:]`. After concatenation, we passed the hidden vector into the fully connected layer after squeezing out the extra dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, dropout, num_layers):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(REVIEW.vocab), embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True)\n",
    "        self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "NUM_LAYERS = 2\n",
    "    \n",
    "model = BiLSTMClassifier(EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, DROPOUT, NUM_LAYERS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
