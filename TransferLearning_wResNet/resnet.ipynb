{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning with ResNet\n",
    "\n",
    "Transfer learning is a powerful technique used in deep learning, where a network trained for one task (general image classification over multiple classes) is adapted to another (classification over just two classes). This allows for a vastly smaller dataset of not more than a few hundred images, and thus for a much faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretrained ResNet-50 model is created using `torchvision.models` subpackage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is not to perform training as it was the case with architectures initialized with random parameters. A pretrained network presumably already knows a lot about the problem it is being applied to and to properly re-train it massive amounts of data are needed. It is precisely the unavailability of such quantities of data that motivate us to reach out for transfer learning technique.\n",
    "\n",
    "What happens instead is the replacement of the standard 1,000-category linear layer doing the ImageNet classification in ResNet-50 architecture with a new block which will be trained for the task at hand, namely classification of images into only two categories. This is achived by freezing all the existing ResNet layers which allows for training only the newly added ones, with the frozen layers still passing activations.\n",
    "\n",
    "IRO `(\"bn\" not in name)` condition - it is preferable not to freeze the BatchNorm layers ina  model as they will be trained to approximate the mean and standard deviation of the dataset that the model was originally trained on, not the dataset that it is being fine-tuned on. For that reason these are also trained alongside the newly added classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in transfer_model.named_parameters():\n",
    "    if (\"bn\" not in name):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier is replaced using the same `nn.Sequential()` chain as it was the case with CNNNet.\n",
    "\n",
    "Parameter `transfer_model.fc.in_features` allows for grabbing the number of activations coming into a layer instead of explicitely stating the number. Same could be done with `out_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.fc = nn.Sequential(\n",
    "    nn.Linear(transfer_model.fc.in_features, 500),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(500, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0\n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(outputs), dim=1)[1], targets).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "        \n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'\n",
    "             .format(epoch, training_loss, valid_loss, num_correct / num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "train_data_path = \"/Users/nikolavetnic/Desktop/Datasets/img_catfish/train/\"\n",
    "train_data = torchvision.datasets.ImageFolder(\n",
    "    root=train_data_path,\n",
    "    transform=img_transforms,\n",
    "    is_valid_file=check_image)\n",
    "\n",
    "val_data_path = \"/Users/nikolavetnic/Desktop/Datasets/img_catfish/val/\"\n",
    "val_data = torchvision.datasets.ImageFolder(\n",
    "    root=val_data_path,\n",
    "    transform=img_transforms,\n",
    "    is_valid_file=check_image)\n",
    "\n",
    "test_data_path = \"/Users/nikolavetnic/Desktop/Datasets/img_catfish/test/\"\n",
    "test_data = torchvision.datasets.ImageFolder(\n",
    "    root=test_data_path,\n",
    "    transform=img_transforms,\n",
    "    is_valid_file=check_image)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    }
   ],
   "source": [
    "print(len(val_data_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.to(device)\n",
    "optimizer = optim.Adam(transfer_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolavetnic/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.28, Validation Loss: 0.21, accuracy = 0.91\n",
      "Epoch: 1, Training Loss: 0.12, Validation Loss: 0.19, accuracy = 0.92\n",
      "Epoch: 2, Training Loss: 0.03, Validation Loss: 0.28, accuracy = 0.87\n",
      "Epoch: 3, Training Loss: 0.07, Validation Loss: 0.44, accuracy = 0.86\n",
      "Epoch: 4, Training Loss: 0.06, Validation Loss: 0.31, accuracy = 0.92\n"
     ]
    }
   ],
   "source": [
    "train(transfer_model, optimizer, torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "labels = ['cat', 'fish']\n",
    "\n",
    "url = \"https://images.theconversation.com/files/243439/original/file-20181101-83635-1xcrr39.jpg\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = img_transforms(img).to(device)\n",
    "\n",
    "predicted_class = labels[torch.argmax(transfer_model(img.unsqueeze(0)))]\n",
    "print(predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Finder\n",
    "\n",
    "Although the learning rate value of 0.001 works well most of the time there is an exact method of determining the best float for our current architecture. The idea is as follows: over the course of an epoch the training starts out with a small learning rate and increases to a higher learning rate over each mini-batch, resulting in a high rate at the end of the epoch. The loss is then calculated for each rate and plotted - by looking at the plot we are searching for the learning rate that gives not the minimum value but the greatest decline.\n",
    "\n",
    "One thing to note is that this function actually trains the model in the process and changes the optimizers learning rates. For that reason the model is to be saved before running it and reloaded afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the parameters using state_dict\n",
    "torch.save(transfer_model.state_dict(), \"/Users/nikolavetnic/Desktop/DeepLearning/TransferLearning_wResNet/resnet_state_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lr(model, loss_fn, optimizer, train_loader, init_value=1e-8, final_value=10.0, device=\"cpu\"):\n",
    "    \n",
    "    number_in_epoch = len(train_loader) - 1\n",
    "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "    best_loss = 0.0\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    \n",
    "    for data in train_loader:\n",
    "        batch_num += 1\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        # crash out if loss explodes\n",
    "        if batch_num > 1 and loss > 4 * best_loss:\n",
    "            if (len(log_lrs) > 20):\n",
    "                return log_lrs[10:-5], losses[10:-5]\n",
    "            else:\n",
    "                return log_lrs, losses\n",
    "        \n",
    "        # record the best loss\n",
    "        if loss < best_loss or batch_num == 1:\n",
    "            best_loss = loss\n",
    "            \n",
    "        # store the values\n",
    "        losses.append(loss.item())\n",
    "        log_lrs.append((lr))\n",
    "        \n",
    "        # backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        lr *= update_step\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "    \n",
    "    if (len(log_lrs) > 20):\n",
    "        return log_lrs[10:-5], losses[10:-5]\n",
    "    else:\n",
    "        return log_lrs, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First bits of training and the last few (especially if the learning rate becomes very large quite quickly) tend to not tell us much information, hence we return slices of `log_lrs` and `losses`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(lrs, losses) = find_lr(transfer_model, torch.nn.CrossEntropyLoss(), optimizer, train_data_loader, device=device)\n",
    "plt.plot(lrs, losses)\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the parameters using state_dict\n",
    "resnet_state_dict = torch.load(\"/Users/nikolavetnic/Desktop/DeepLearning/TransferLearning_wResNet/resnet_state_dict\")\n",
    "transfer_model.load_state_dict(resnet_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom transforms\n",
    "\n",
    "Here a transform that will randomly change the image color space from RGB into HSV. PIL library provides `Image.convert()` which we can use to translate an image from one color space to another. The custom transform function will be then wrapped in a `transforms.Lambda` provided by PyTorch and thus made available to the transform pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _random_color_space(x):\n",
    "    output = x.convert(\"HSV\")\n",
    "    return output\n",
    "\n",
    "color_transform = transforms.Lambda(lambda x: _random_color_space(x))\n",
    "\n",
    "random_color_transform = torchvision.transforms.RandomApply([color_transform])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case a lambda isn't enought it is possible to create a custom transform that operates on either PIL image data or a tensor. Such a class has to implement two methods: `__call__`, which transform pipeline will invoke during the transformation process, and `__repr__`, which should return a string representation of the transform, along with any state that may be useful for diagnostics.\n",
    "\n",
    "`torch.zeros_like()` returns a tensor filled with the scalar value `0`, with the same size as `input`. `torch.zeros_like(input)` is equivalent to `torch.zeros(input.size(), dtype=input.dtype, layout=input.layout, device=input.device)`.\n",
    "\n",
    "`normal_` fills the given 2-dimensional matrix with values drawn from a normal distribution parameterized by `mean` and `std`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise():\n",
    "    \"\"\"Adds gaussian noise to a tensor.\n",
    "    \n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>>     Noise(0.1, 0.05)),\n",
    "        >>> ])\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mean, stddev):\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.zeros_like(tensor).normal_(self.mean, self.stddev)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        repr = f\"{self.__class__.__name__ }(mean={self.mean},sttdev={self.stddev})\"\n",
    "        return repr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transform_pipeline = transforms.Compose([random_color_transform, Noise(0.1, 0.05)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles\n",
    "\n",
    "Given a list of models, we can produce predictions for each model and then make an average to make a final prediction.\n",
    "\n",
    "This is something I have to research further as I have just copied the code from the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikolavetnic/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "models_ensemble = [models.resnet50().to(device), models.resnet50().to(device)]\n",
    "predictions = [F.softmax(m(torch.rand(1,3,224,244).to(device))) for m in models_ensemble] \n",
    "avg_prediction = torch.stack(predictions).mean(0).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(721)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0009, 0.0020, 0.0003,  ..., 0.0005, 0.0004, 0.0008]],\n",
       "\n",
       "        [[0.0015, 0.0002, 0.0011,  ..., 0.0004, 0.0010, 0.0010]]],\n",
       "       grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
